{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable  \n",
    "import torch.utils.data as Data  \n",
    "import time\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 1  \n",
    "BATCH_SIZE = 50  \n",
    "LR = 3e-4\n",
    "\n",
    "def grid_arr(arr):\n",
    "    ret = np.zeros(shape=(4,4)+(16,),dtype =bool)\n",
    "    for r in range(4):\n",
    "        for c in range(4):\n",
    "            ret[r, c, map_table[arr[r, c]]] = 1\n",
    "    return ret\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential( #input shape (4,4,16)\n",
    "                        nn.Conv2d(in_channels=16, #input height \n",
    "                        out_channels=60, #n_filter\n",
    "                        kernel_size=(4,1), #filter size\n",
    "                        stride=1, #filter step\n",
    "                        padding=1 #con2d出来的图片大小不变\n",
    "                        ), #output shape (60, 3, 6)\n",
    "                        nn.ReLU(),\n",
    "                        nn.MaxPool2d(kernel_size=1) \n",
    "             \n",
    "           )\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(60, 60, (1,4), 1, 1), #output shape (60, 4, 4)\n",
    "                                    nn.ReLU(),\n",
    "                                     nn.MaxPool2d(1))\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(60, 60, (2,2), 1, 0), #output shape (60, 3, 3)\n",
    "                                    nn.ReLU(),\n",
    "                                     nn.MaxPool2d(1))\n",
    "        self.conv4 = nn.Sequential(nn.Conv2d(60, 60, (3,3), 1, 1), #output shape (60, 3, 3)\n",
    "                                    nn.ReLU(),\n",
    "                                     nn.MaxPool2d(1))\n",
    "        self.conv5 = nn.Sequential(nn.Conv2d(60, 60, (4,4), 1, 1), #output shape (60, 2, 2)\n",
    "                                    nn.ReLU(),\n",
    "                                     nn.MaxPool2d(1))\n",
    "        self.out = nn.Linear(60*4*4,4)\n",
    "       \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = x.view(x.size(0), -1)  #flat (batch_size, 32*7*7)\n",
    "        output = self.out(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "cnn = CNN()\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)  \n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "pick_file = open('training_data.pkl', 'rb')\n",
    "dataset = pickle.load(pick_file)\n",
    "data_x = []\n",
    "data_y = []\n",
    "for i in len(dataset):\n",
    "    data_x.append(dataset[i][0])\n",
    "    data_y.append(dataset[i][1])\n",
    "\n",
    "for epoch in range(EPOCH):  \n",
    "    start = time.time() \n",
    "    for i in range(len(dataset)):  \n",
    "        b_x = Variable(grid_arr(data_x), requires_grad=False) \n",
    "        b_y = Variable(data_y, requires_grad=False)  \n",
    "  \n",
    "        output = cnn(b_x)  \n",
    "        loss = loss_function(output, b_y)  \n",
    "        optimizer.zero_grad()  \n",
    "        loss.backward()  \n",
    "        optimizer.step()  \n",
    "  \n",
    "        if step % 100 == 0:  \n",
    "            print('Epoch:', epoch, '|Step:', step,  \n",
    "                  '|train loss:%.4f'%loss.data[0])  \n",
    "    duration = time.time() - start \n",
    "    print('Training duation: %.4f'%duration)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module torch.utils.data.dataloader in torch.utils.data:\n",
      "\n",
      "NAME\n",
      "    torch.utils.data.dataloader\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        DataLoader\n",
      "        ExceptionWrapper\n",
      "        ManagerWatchdog\n",
      "    \n",
      "    class DataLoader(builtins.object)\n",
      "     |  Data loader. Combines a dataset and a sampler, and provides\n",
      "     |  single- or multi-process iterators over the dataset.\n",
      "     |  \n",
      "     |  Arguments:\n",
      "     |      dataset (Dataset): dataset from which to load the data.\n",
      "     |      batch_size (int, optional): how many samples per batch to load\n",
      "     |          (default: ``1``).\n",
      "     |      shuffle (bool, optional): set to ``True`` to have the data reshuffled\n",
      "     |          at every epoch (default: ``False``).\n",
      "     |      sampler (Sampler, optional): defines the strategy to draw samples from\n",
      "     |          the dataset. If specified, ``shuffle`` must be False.\n",
      "     |      batch_sampler (Sampler, optional): like sampler, but returns a batch of\n",
      "     |          indices at a time. Mutually exclusive with :attr:`batch_size`,\n",
      "     |          :attr:`shuffle`, :attr:`sampler`, and :attr:`drop_last`.\n",
      "     |      num_workers (int, optional): how many subprocesses to use for data\n",
      "     |          loading. 0 means that the data will be loaded in the main process.\n",
      "     |          (default: ``0``)\n",
      "     |      collate_fn (callable, optional): merges a list of samples to form a mini-batch.\n",
      "     |      pin_memory (bool, optional): If ``True``, the data loader will copy tensors\n",
      "     |          into CUDA pinned memory before returning them.\n",
      "     |      drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,\n",
      "     |          if the dataset size is not divisible by the batch size. If ``False`` and\n",
      "     |          the size of dataset is not divisible by the batch size, then the last batch\n",
      "     |          will be smaller. (default: ``False``)\n",
      "     |      timeout (numeric, optional): if positive, the timeout value for collecting a batch\n",
      "     |          from workers. Should always be non-negative. (default: ``0``)\n",
      "     |      worker_init_fn (callable, optional): If not ``None``, this will be called on each\n",
      "     |          worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as\n",
      "     |          input, after seeding and before data loading. (default: ``None``)\n",
      "     |  \n",
      "     |  .. note:: By default, each worker will have its PyTorch seed set to\n",
      "     |            ``base_seed + worker_id``, where ``base_seed`` is a long generated\n",
      "     |            by main process using its RNG. However, seeds for other libraies\n",
      "     |            may be duplicated upon initializing workers (w.g., NumPy), causing\n",
      "     |            each worker to return identical random numbers. (See\n",
      "     |            :ref:`dataloader-workers-random-seed` section in FAQ.) You may\n",
      "     |            use :func:`torch.initial_seed()` to access the PyTorch seed for\n",
      "     |            each worker in :attr:`worker_init_fn`, and use it to set other\n",
      "     |            seeds before data loading.\n",
      "     |  \n",
      "     |  .. warning:: If ``spawn`` start method is used, :attr:`worker_init_fn` cannot be an\n",
      "     |               unpicklable object, e.g., a lambda function.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=<function default_collate at 0x7f8930d64f28>, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  __setattr__(self, attr, val)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class ExceptionWrapper(builtins.object)\n",
      "     |  Wraps an exception plus traceback to communicate across threads\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, exc_info)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class ManagerWatchdog(builtins.object)\n",
      "     |  # On Windows, the parent ID of the worker process remains unchanged when the manager process\n",
      "     |  # is gone, and the only way to check it through OS is to let the worker have a process handle\n",
      "     |  # of the manager and ask if the process status has changed.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  is_alive(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    default_collate(batch)\n",
      "        Puts each data field into a tensor with outer dimension batch size\n",
      "    \n",
      "    pin_memory_batch(batch)\n",
      "\n",
      "DATA\n",
      "    IS_WINDOWS = False\n",
      "    MP_STATUS_CHECK_INTERVAL = 5.0\n",
      "    numpy_type_map = {'float16': <class 'torch.HalfTensor'>, 'float32': <c...\n",
      "    string_classes = (<class 'str'>, <class 'bytes'>)\n",
      "\n",
      "FILE\n",
      "    /home/sacha/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Data.dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
